{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Magentic Example",
   "id": "9d7467914cfa45c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T13:04:20.336166Z",
     "start_time": "2024-10-20T13:04:20.307177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from magentic.chat_model.openai_chat_model import OpenaiChatModel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = OpenaiChatModel(\n",
    "    api_key=os.getenv('OPENAI_API_KEY'),\n",
    "    base_url=os.getenv('OPENAI_API_ENDPOINT'),\n",
    "    api_type=\"openai\",\n",
    "    model=\"gpt-4o\",\n",
    ")"
   ],
   "id": "82b1185eaeec8f9f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T13:04:53.931487Z",
     "start_time": "2024-10-20T13:04:53.877386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# https://magentic.dev/logging-and-tracing/\n",
    "import logfire\n",
    "\n",
    "logfire.configure(send_to_logfire=\"if-token-present\")\n",
    "logfire.instrument_openai()\n",
    "logfire.info(\"Hello, world!\")"
   ],
   "id": "ee52d379d74052ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:04:53.929 Hello, world!\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## @prompt",
   "id": "1a850cab31168fa2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-20T13:05:06.006548Z",
     "start_time": "2024-10-20T13:05:04.592473Z"
    }
   },
   "source": [
    "from magentic import prompt\n",
    "\n",
    "\n",
    "@prompt('Add more \"dude\"ness to: {phrase}', model=chat_model)\n",
    "def dudeify(phrase: str) -> str: ...  # No function body as this is never executed\n",
    "\n",
    "\n",
    "dudeify(\"Hello, how are you?\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:05:04.594 Calling prompt-function dudeify\n",
      "13:05:04.597   Chat Completion with 'gpt-4o' [LLM]\n",
      "13:05:06.001   streaming response from 'gpt-4o' took 0.18s [LLM]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hey dude, how's it going?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T13:05:55.322778Z",
     "start_time": "2024-10-20T13:05:52.675180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from magentic import prompt\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Superhero(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    power: str\n",
    "    enemies: list[str]\n",
    "\n",
    "\n",
    "@prompt(\"Create a Superhero named {name}.\", model=chat_model) \n",
    "def create_superhero(name: str) -> Superhero: ...\n",
    "\n",
    "\n",
    "create_superhero(\"Garden Man\")\n",
    "# Superhero(name='Garden Man', age=30, power='Control over plants', enemies=['Pollution Man', 'Concrete Woman'])"
   ],
   "id": "25a96e9a4fec2d1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:05:52.680 Calling prompt-function create_superhero\n",
      "13:05:52.686   Chat Completion with 'gpt-4o' [LLM]\n",
      "13:05:55.317   streaming response from 'gpt-4o' took 0.26s [LLM]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Superhero(name='Garden Man', age=35, power='Control over plant life', enemies=['Weed Whacker', 'Deforestation Duo', 'Concrete Crusher'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## @chatprompt",
   "id": "ae54a22e5b4f125f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T13:12:45.082827Z",
     "start_time": "2024-10-20T13:12:43.614165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from magentic import chatprompt, AssistantMessage, SystemMessage, UserMessage\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Quote(BaseModel):\n",
    "    quote: str\n",
    "    character: str\n",
    "\n",
    "\n",
    "@chatprompt(\n",
    "    SystemMessage(\"You are a movie buff.\"),\n",
    "    UserMessage(\"What is your favorite quote from Harry Potter?\"),\n",
    "    AssistantMessage(\n",
    "        Quote(\n",
    "            quote=\"It does not do to dwell on dreams and forget to live.\",\n",
    "            character=\"Albus Dumbledore\",\n",
    "        )\n",
    "    ),\n",
    "    UserMessage(\"What is your favorite quote from {movie}?\"),\n",
    "    model=chat_model,\n",
    ")\n",
    "def get_movie_quote(movie: str) -> Quote: ...\n",
    "\n",
    "\n",
    "get_movie_quote(\"Iron Man\")\n",
    "# Quote(quote='I am Iron Man.', character='Tony Stark')"
   ],
   "id": "40c297ec1bec4bec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:12:43.629 Calling chatprompt-function get_movie_quote\n",
      "13:12:43.638   Chat Completion with 'gpt-4o' [LLM]\n",
      "13:12:45.080   streaming response from 'gpt-4o' took 0.16s [LLM]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Quote(quote='I am Iron Man.', character='Tony Stark')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FunctionCall",
   "id": "cfee9a8465eb2166"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T13:15:47.331025Z",
     "start_time": "2024-10-20T13:15:45.758657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Literal\n",
    "\n",
    "from magentic import prompt, FunctionCall\n",
    "\n",
    "\n",
    "def search_twitter(query: str, category: Literal[\"latest\", \"people\"]) -> str:\n",
    "    \"\"\"Searches Twitter for a query.\"\"\"\n",
    "    print(f\"Searching Twitter for {query!r} in category {category!r}\")\n",
    "    return \"<twitter results>\"\n",
    "\n",
    "\n",
    "def search_youtube(query: str, channel: str = \"all\") -> str:\n",
    "    \"\"\"Searches YouTube for a query.\"\"\"\n",
    "    print(f\"Searching YouTube for {query!r} in channel {channel!r}\")\n",
    "    return \"<youtube results>\"\n",
    "\n",
    "\n",
    "@prompt(\n",
    "    \"Use the appropriate search function to answer: {question}\",\n",
    "    functions=[search_twitter, search_youtube],\n",
    "    model=chat_model,\n",
    ")\n",
    "def perform_search(question: str) -> FunctionCall[str]: ...\n",
    "\n",
    "\n",
    "output = perform_search(\"What is the latest news on LLMs?\")\n",
    "print(output)\n",
    "# > FunctionCall(<function search_twitter at 0x10c367d00>, 'LLMs', 'latest')\n",
    "output()\n",
    "# > Searching Twitter for 'Large Language Models news' in category 'latest'\n",
    "# '<twitter results>'"
   ],
   "id": "ba191fa48633c07f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:15:45.765 Calling prompt-function perform_search\n",
      "13:15:45.775   Chat Completion with 'gpt-4o' [LLM]\n",
      "13:15:47.319   streaming response from 'gpt-4o' took 0.17s [LLM]\n",
      "FunctionCall(<function search_twitter at 0x1100436a0>, 'latest news on LLMs', 'latest')\n",
      "13:15:47.327 Executing function call search_twitter\n",
      "Searching Twitter for 'latest news on LLMs' in category 'latest'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<twitter results>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## @prompt_chain",
   "id": "77f9126ccec62d64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T13:19:57.480835Z",
     "start_time": "2024-10-20T13:19:55.218009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from magentic import prompt_chain\n",
    "\n",
    "\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    # Pretend to query an API\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "\n",
    "\n",
    "@prompt_chain(\n",
    "    \"What's the weather like in {city}?\",\n",
    "    functions=[get_current_weather],\n",
    "    model=chat_model,\n",
    ")\n",
    "def describe_weather(city: str) -> str: ...\n",
    "\n",
    "\n",
    "describe_weather(\"Boston\")\n",
    "# 'The current weather in Boston is 72°F and it is sunny and windy.'"
   ],
   "id": "4ba66ef1f55d97b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:19:55.230 Calling prompt-chain describe_weather\n",
      "13:19:55.238   Chat Completion with 'gpt-4o' [LLM]\n",
      "13:19:56.590   streaming response from 'gpt-4o' took 0.04s [LLM]\n",
      "13:19:56.590   Executing function call get_current_weather\n",
      "13:19:56.597   Chat Completion with 'gpt-4o' [LLM]\n",
      "13:19:57.477   streaming response from 'gpt-4o' took 0.24s [LLM]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The weather in Boston is currently 72°F with sunny and windy conditions.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ParallelFunctionCall with @chatprompt",
   "id": "32677717c622359d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T13:26:18.954614Z",
     "start_time": "2024-10-20T13:26:17.149969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from magentic import (\n",
    "    chatprompt,\n",
    "    AssistantMessage,\n",
    "    FunctionCall,\n",
    "    FunctionResultMessage,\n",
    "    ParallelFunctionCall,\n",
    "    UserMessage,\n",
    ")\n",
    "\n",
    "\n",
    "def plus(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def minus(a: int, b: int) -> int:\n",
    "    return a - b\n",
    "\n",
    "\n",
    "plus_1_2 = FunctionCall(plus, 1, 2)\n",
    "minus_2_1 = FunctionCall(minus, 2, 1)\n",
    "\n",
    "\n",
    "@chatprompt(\n",
    "    UserMessage(\n",
    "        \"Sum 1 and 2. Also subtract 1 from 2.\",\n",
    "    ),\n",
    "    AssistantMessage(ParallelFunctionCall([plus_1_2, minus_2_1])),\n",
    "    FunctionResultMessage(3, plus_1_2),\n",
    "    FunctionResultMessage(1, minus_2_1),\n",
    "    UserMessage(\"Now add 4 to both results.\"),\n",
    "    functions=[plus, minus],\n",
    "    model=chat_model,\n",
    ")\n",
    "def do_math() -> ParallelFunctionCall[int]: ...\n",
    "\n",
    "\n",
    "output = do_math()\n",
    "print(list(output))\n",
    "# > [FunctionCall(<function plus at 0x10c3584c0>, 3, 4),\n",
    "#    FunctionCall(<function plus at 0x10c3584c0>, 1, 4)]\n",
    "output()\n",
    "# (7, 5)"
   ],
   "id": "2a8ce33f03afca8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:26:17.151 Calling chatprompt-function do_math\n",
      "13:26:17.158   Chat Completion with 'gpt-4o' [LLM]\n",
      "13:26:18.949 streaming response from 'gpt-4o' took 0.01s [LLM]\n",
      "[FunctionCall(<function plus at 0x110c40680>, 3, 4), FunctionCall(<function plus at 0x110c40680>, 1, 4)]\n",
      "13:26:18.950 Executing parallel function call\n",
      "13:26:18.951   Executing function call plus\n",
      "13:26:18.951   Executing function call plus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f0c9186ba85a24e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
