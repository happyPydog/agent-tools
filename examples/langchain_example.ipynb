{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:41:20.421864Z",
     "start_time": "2024-10-21T06:41:20.402539Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_ENDPOINT = os.getenv(\"OPENAI_API_ENDPOINT\")\n",
    "MODEL = \"gpt-4o\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base LangChain Usage"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:41:28.639026Z",
     "start_time": "2024-10-21T06:41:26.091160Z"
    }
   },
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_ENDPOINT, model=MODEL)\n",
    "\n",
    "llm.invoke(\"hi\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: hi\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[llm:ChatOpenAI] [2.51s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Hello! How can I help you today?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Hello! How can I help you today?\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 9,\n",
      "                \"prompt_tokens\": 8,\n",
      "                \"total_tokens\": 17,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_67802d9a6d\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-93cf1d9b-6032-4292-bef4-ab66ff5dd5db-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 8,\n",
      "              \"output_tokens\": 9,\n",
      "              \"total_tokens\": 17,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 9,\n",
      "      \"prompt_tokens\": 8,\n",
      "      \"total_tokens\": 17,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_67802d9a6d\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_67802d9a6d', 'finish_reason': 'stop', 'logprobs': None}, id='run-93cf1d9b-6032-4292-bef4-ab66ff5dd5db-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:41:33.901396Z",
     "start_time": "2024-10-21T06:41:29.696754Z"
    }
   },
   "source": [
    "from typing import cast\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# https://aclanthology.org/2024.tacl-1.10/\n",
    "# Title: Red Teaming Language Model Detectors with Language Models\n",
    "abstract = (\n",
    "    \"The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. \"\n",
    "    \"To prevent the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs. \"\n",
    "    \"In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. \"\n",
    "    \"We study two types of attack strategies: \"\n",
    "    \"1) replacing certain words in an LLM’s output with their synonyms given the context; \"\n",
    "    \"2) automatically searching for an instructional prompt to alter the writing style of the generation. \"\n",
    "    \"In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. \"\n",
    "    \"Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. \"\n",
    "    \"Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, \"\n",
    "    \"underscoring the urgent need to improve the robustness of LLM-generated text detection systems. \"\n",
    "    \"Code is available at https://github.com/shizhouxing/LLM-Detector-Robustness.\"\n",
    ")\n",
    "\n",
    "\n",
    "class AbstractResult(BaseModel):\n",
    "    content: str = Field(..., title=\"Summary of the paper abstract in Traditional Chinese\")\n",
    "\n",
    "\n",
    "def summary_paper_abstract(llm: ChatOpenAI, abstract: str) -> AbstractResult:\n",
    "    prompt = ChatPromptTemplate(\n",
    "        [\n",
    "            (\"system\", \"You are a AI researcher.\"),\n",
    "            (\"human\", \"Summary the paper abstract in Traditional Chinese.\"),\n",
    "            (\"human\", \"Paper Abstract:```{abstract}```\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = llm.with_structured_output(AbstractResult).with_retry(stop_after_attempt=6)\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke(input={\"abstract\": abstract})\n",
    "    return cast(AbstractResult, response)\n",
    "\n",
    "\n",
    "summary_paper_abstract(llm, abstract)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"abstract\": \"The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. We study two types of attack strategies: 1) replacing certain words in an LLM’s output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems. Code is available at https://github.com/shizhouxing/LLM-Detector-Robustness.\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m{\n",
      "  \"abstract\": \"The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. We study two types of attack strategies: 1) replacing certain words in an LLM’s output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems. Code is available at https://github.com/shizhouxing/LLM-Detector-Robustness.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableSequence > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableSequence > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a AI researcher.\\nHuman: Summary the paper abstract in Traditional Chinese.\\nHuman: Paper Abstract:```The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. We study two types of attack strategies: 1) replacing certain words in an LLM’s output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems. Code is available at https://github.com/shizhouxing/LLM-Detector-Robustness.```\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableSequence > chain:RunnableSequence > llm:ChatOpenAI] [4.08s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_NzPw96YfxBV4A5tAmlXc1rP3\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"content\\\":\\\"大型語言模型（LLM）的普及和強大能力，如果被惡意用戶利用，將帶來顯著的安全和倫理風險。為了防止LLM的潛在欺騙性使用，近期的研究提出了檢測LLM生成文本和保護LLM的算法。在本文中，我們調查了這些LLM檢測器在對抗攻擊下的穩健性和可靠性。我們研究了兩種攻擊策略：1）在給定上下文的情況下，用同義詞替換LLM輸出中的某些詞；2）自動搜索指令提示以改變生成的寫作風格。在這兩種策略中，我們利用輔助LLM來生成詞替換或指令提示。與之前的工作不同，我們考慮了一種具有挑戰性的設置，即輔助LLM也可以受到檢測器的保護。實驗結果顯示，我們的攻擊有效地損害了研究中的所有檢測器的性能，生成了合理的文本，強調了提高LLM生成文本檢測系統穩健性的迫切需要。代碼可在https://github.com/shizhouxing/LLM-Detector-Robustness獲得。\\\"}\",\n",
      "                    \"name\": \"AbstractResult\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 297,\n",
      "                \"prompt_tokens\": 290,\n",
      "                \"total_tokens\": 587,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_67802d9a6d\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-79a309fd-4bc7-4f49-bafe-7e94d71e11ea-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"AbstractResult\",\n",
      "                \"args\": {\n",
      "                  \"content\": \"大型語言模型（LLM）的普及和強大能力，如果被惡意用戶利用，將帶來顯著的安全和倫理風險。為了防止LLM的潛在欺騙性使用，近期的研究提出了檢測LLM生成文本和保護LLM的算法。在本文中，我們調查了這些LLM檢測器在對抗攻擊下的穩健性和可靠性。我們研究了兩種攻擊策略：1）在給定上下文的情況下，用同義詞替換LLM輸出中的某些詞；2）自動搜索指令提示以改變生成的寫作風格。在這兩種策略中，我們利用輔助LLM來生成詞替換或指令提示。與之前的工作不同，我們考慮了一種具有挑戰性的設置，即輔助LLM也可以受到檢測器的保護。實驗結果顯示，我們的攻擊有效地損害了研究中的所有檢測器的性能，生成了合理的文本，強調了提高LLM生成文本檢測系統穩健性的迫切需要。代碼可在https://github.com/shizhouxing/LLM-Detector-Robustness獲得。\"\n",
      "                },\n",
      "                \"id\": \"call_NzPw96YfxBV4A5tAmlXc1rP3\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 290,\n",
      "              \"output_tokens\": 297,\n",
      "              \"total_tokens\": 587,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 297,\n",
      "      \"prompt_tokens\": 290,\n",
      "      \"total_tokens\": 587,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_67802d9a6d\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableSequence > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableSequence > chain:RunnableSequence > parser:PydanticToolsParser] [1ms] Exiting Parser run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableSequence > chain:RunnableSequence] [4.09s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableSequence] [4.09s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [4.09s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AbstractResult(content='大型語言模型（LLM）的普及和強大能力，如果被惡意用戶利用，將帶來顯著的安全和倫理風險。為了防止LLM的潛在欺騙性使用，近期的研究提出了檢測LLM生成文本和保護LLM的算法。在本文中，我們調查了這些LLM檢測器在對抗攻擊下的穩健性和可靠性。我們研究了兩種攻擊策略：1）在給定上下文的情況下，用同義詞替換LLM輸出中的某些詞；2）自動搜索指令提示以改變生成的寫作風格。在這兩種策略中，我們利用輔助LLM來生成詞替換或指令提示。與之前的工作不同，我們考慮了一種具有挑戰性的設置，即輔助LLM也可以受到檢測器的保護。實驗結果顯示，我們的攻擊有效地損害了研究中的所有檢測器的性能，生成了合理的文本，強調了提高LLM生成文本檢測系統穩健性的迫切需要。代碼可在https://github.com/shizhouxing/LLM-Detector-Robustness獲得。')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
