{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"@prompt\"\"\"\n",
    "\n",
    "import os\n",
    "import abc\n",
    "from collections.abc import Callable\n",
    "from enum import Enum\n",
    "from functools import update_wrapper\n",
    "import inspect\n",
    "import types\n",
    "from typing import (\n",
    "    Any,\n",
    "    Generic,\n",
    "    Iterable,\n",
    "    Literal,\n",
    "    Never,\n",
    "    ParamSpec,\n",
    "    Protocol,\n",
    "    Sequence,\n",
    "    TypeVar,\n",
    "    TypedDict,\n",
    "    Union,\n",
    "    Unpack,\n",
    "    cast,\n",
    "    get_args,\n",
    "    get_origin,\n",
    "    overload,\n",
    ")\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI as LangchainChatOpenAI\n",
    "from langchain_openai.chat_models import AzureChatOpenAI as LangchainAzureChatOpenAI\n",
    "from langfuse.callback import CallbackHandler as LangfuseCallbackHandler\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Role(Enum):\n",
    "    \"\"\"Role of conversation.\"\"\"\n",
    "\n",
    "    SYSTEM = \"system\"\n",
    "    HUMAN = \"human\"\n",
    "    AI = \"ai\"\n",
    "\n",
    "\n",
    "class Message(TypedDict):\n",
    "    role: Role\n",
    "    content: str\n",
    "\n",
    "\n",
    "class LangfuseCallbackConfig(TypedDict, total=False):\n",
    "    public_key: str | None\n",
    "    secret_key: str | None\n",
    "    host: str | None\n",
    "    debug: bool\n",
    "    update_stateful_client: bool\n",
    "    session_id: str | None\n",
    "    user_id: str | None\n",
    "    trace_name: str | None\n",
    "    release: str | None\n",
    "    version: str | None\n",
    "    metadata: dict[str, any] | None\n",
    "    tags: list[str] | None\n",
    "    threads: int | None\n",
    "    flush_at: int | None\n",
    "    flush_interval: int | None\n",
    "    max_retries: int | None\n",
    "    timeout: int | None\n",
    "    enabled: bool | None\n",
    "    sdk_integration: str | None\n",
    "    sample_rate: float | None\n",
    "\n",
    "\n",
    "P = ParamSpec(\"P\")\n",
    "R = TypeVar(\"R\")\n",
    "TypeT = TypeVar(\"TypeT\", bound=type)\n",
    "MessageLikeType = Union[str, Iterable[tuple[Role, str] | tuple[str, str] | Message]]\n",
    "ModelType = Union[LangchainChatOpenAI, LangchainAzureChatOpenAI, OpenAI, AzureOpenAI]\n",
    "\n",
    "\n",
    "class PromptFunction(Generic[P, R]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        parameters: Sequence[inspect.Parameter],\n",
    "        return_type: type[R],\n",
    "        messages: Sequence[MessageLikeType],\n",
    "        functions: list[Callable[..., Any]] | None = None,\n",
    "        stop: list[str] | None = None,\n",
    "        max_retries: int = 0,\n",
    "        model: ModelType | None = None,\n",
    "    ):\n",
    "        self._name = name\n",
    "        self._signature = inspect.Signature(\n",
    "            parameters=parameters,\n",
    "            return_annotation=return_type,\n",
    "        )\n",
    "        self._messages = messages\n",
    "        self._functions = functions or []\n",
    "        self._stop = stop\n",
    "        self._max_retries = max_retries\n",
    "        self._model = model\n",
    "\n",
    "        self._return_types = list(self.split_union_type(return_type))\n",
    "\n",
    "    def is_union_type(self, type_: type) -> bool:\n",
    "        type_ = get_origin(type_) or type_\n",
    "        return type_ is Union or type_ is types.UnionType\n",
    "\n",
    "    def split_union_type(self, type_: TypeT) -> Sequence[TypeT]:\n",
    "        return get_args(type_) if self.is_union_type(type_) else [type_]\n",
    "\n",
    "\n",
    "class OpenAIPromptFunction(PromptFunction):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        parameters: Sequence[inspect.Parameter],\n",
    "        return_type: type[R],\n",
    "        messages: Sequence[MessageLikeType],\n",
    "        functions: list[Callable[..., Any]] | None = None,\n",
    "        stop: list[str] | None = None,\n",
    "        max_retries: int = 0,\n",
    "        model: ModelType | None = None,\n",
    "        model_name: Literal[\"gpt-4o\", \"gpt-4o-mini\", \"gpt\"] | None = None,\n",
    "        langfuse_config: Unpack[LangfuseCallbackConfig] | None = None,\n",
    "    ) -> None:\n",
    "        self._model_name = model_name\n",
    "        self._langfuse_config = langfuse_config\n",
    "        super().__init__(name, parameters, return_type, messages, functions, stop, max_retries, model)\n",
    "\n",
    "    def __call__(self, *args: P.args, **kwargs: P.kwargs) -> R:\n",
    "        print(f\"{self._messages = }\")\n",
    "        print(f\"{args = }\")\n",
    "        print(f\"{kwargs = }\")\n",
    "        bound_args = self._signature.bind(*args, **kwargs)\n",
    "        bound_args.apply_defaults()\n",
    "        print(f\"Bound arguments: {bound_args.arguments}\")\n",
    "\n",
    "        messages = self._format(self._messages, args)\n",
    "        print(f\"Formatted messages: {messages}\")\n",
    "        model = self._get_model()\n",
    "\n",
    "        config = {}\n",
    "        config.update(self._langfuse_config or {})\n",
    "\n",
    "        resp = model.chat.completions.create(messages=messages, model=self._model_name, **config, response_format=...)\n",
    "\n",
    "        return self._model.chat()\n",
    "\n",
    "    def run(self, model: OpenAI | AzureOpenAI, messages: list[MessageLikeType]) -> str: ...\n",
    "\n",
    "    def _get_model(self) -> OpenAI:\n",
    "        if not is_openai_model(self._model):\n",
    "            raise ValueError(\"Invalid OpenAI model.\")\n",
    "        return cast(OpenAI, self._model)\n",
    "\n",
    "    def format(self, *args: P.args, **kwargs: P.kwargs) -> list[MessageLikeType]:\n",
    "        bound_args = self._signature.bind(*args, **kwargs)\n",
    "        bound_args.apply_defaults()\n",
    "        formatted_messages: list[MessageLikeType] = []\n",
    "        for message_template in self._messages:\n",
    "            if isinstance(message_template, str):\n",
    "                formatted_messages.append(message_template.format(**bound_args.arguments))\n",
    "            else:\n",
    "                # Assuming message_template is iterable of tuples or Message\n",
    "                formatted_message = []\n",
    "                for item in message_template:\n",
    "                    if isinstance(item, tuple):\n",
    "                        formatted_message.append(\n",
    "                            tuple(arg.format(**bound_args.arguments) if isinstance(arg, str) else arg for arg in item)\n",
    "                        )\n",
    "                    elif isinstance(item, dict):  # Message TypedDict\n",
    "                        formatted_message.append(\n",
    "                            {\"role\": item[\"role\"], \"content\": item[\"content\"].format(**bound_args.arguments)}\n",
    "                        )\n",
    "                formatted_messages.append(formatted_message)\n",
    "        return formatted_messages\n",
    "\n",
    "\n",
    "class LangchainOpenAIPromptFunction(PromptFunction):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        parameters: Sequence[inspect.Parameter],\n",
    "        return_type: type[R],\n",
    "        messages: Sequence,\n",
    "        llm: LangchainChatOpenAI | LangchainAzureChatOpenAI,\n",
    "        callbacks: LangfuseCallbackHandler,\n",
    "    ) -> None:\n",
    "        self._callbacks = callbacks\n",
    "        super().__init__(name, parameters, return_type, messages, llm)\n",
    "\n",
    "\n",
    "class PromptDecorator(Protocol):\n",
    "\n",
    "    def __call__(self, func: Callable[P, R]) -> PromptFunction[P, R]: ...\n",
    "\n",
    "\n",
    "def env_error(env_var: str) -> Never:\n",
    "    raise ValueError(f\"{env_var} environment variable is not set.\")\n",
    "\n",
    "\n",
    "def is_openai_model(model: ModelType) -> bool:\n",
    "    return isinstance(model, OpenAI) or isinstance(model, AzureOpenAI)\n",
    "\n",
    "\n",
    "def is_langchain_model(model: ModelType) -> bool:\n",
    "    return isinstance(model, LangchainChatOpenAI) or isinstance(model, LangchainAzureChatOpenAI)\n",
    "\n",
    "\n",
    "def get_openai_model() -> OpenAI:\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if api_key is None:\n",
    "        env_error(\"OPENAI_API_KEY\")\n",
    "\n",
    "    base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "    if base_url is None:\n",
    "        env_error(\"OPENAI_BASE_URL\")\n",
    "\n",
    "    return OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "\n",
    "def get_langchain_openai_model() -> LangchainChatOpenAI:\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    if api_key is None:\n",
    "        env_error(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    if azure_endpoint is None:\n",
    "        env_error(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "    if api_version is None:\n",
    "        env_error(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "    model = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "    if model is None:\n",
    "        env_error(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "\n",
    "    return LangchainChatOpenAI(\n",
    "        api_key=api_key,\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_version=api_version,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "\n",
    "def prompt(\n",
    "    *messages: Sequence[MessageLikeType],\n",
    "    model: ModelType | None = None,\n",
    "    model_name: str | None = None,\n",
    "    langfuse_config: Unpack[LangfuseCallbackConfig] | None = None,\n",
    "    temperature: float = 0.0,\n",
    ") -> PromptDecorator:\n",
    "    model = model or get_openai_model()\n",
    "\n",
    "    def decorator(func: Callable[P, R]) -> PromptFunction[P, R]:\n",
    "        func_signature = inspect.signature(func)\n",
    "        print(f\"Function signature: {func_signature}\")\n",
    "        print(func_signature.parameters)\n",
    "        print(f\"Function return type: {func_signature.return_annotation}\")\n",
    "\n",
    "        if is_langchain_model(model):\n",
    "            prompt_function = LangchainOpenAIPromptFunction(\n",
    "                name=func.__name__,\n",
    "                parameters=func_signature.parameters.values(),\n",
    "                messages=messages,\n",
    "                model=model,\n",
    "                langfuse_config=langfuse_config,\n",
    "            )\n",
    "            return cast(LangchainOpenAIPromptFunction, update_wrapper(prompt_function, func))\n",
    "\n",
    "        prompt_function = OpenAIPromptFunction(\n",
    "            name=func.__name__,\n",
    "            parameters=func_signature.parameters.values(),\n",
    "            return_type=func_signature.return_annotation,\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "        )\n",
    "        return cast(OpenAIPromptFunction, update_wrapper(prompt_function, func))\n",
    "\n",
    "    return cast(PromptDecorator, decorator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function signature: (text: str) -> __main__.foo\n",
      "OrderedDict({'text': <Parameter \"text: str\">})\n",
      "Function return type: <class '__main__.foo'>\n",
      "self._messages = ('Translate the given text to Chinese.\\n', 'TEXT: ```{text}```')\n",
      "args = ('Hello, how are you?',)\n",
      "kwargs = {}\n",
      "Bound arguments: {'text': 'Hello, how are you?'}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bound_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 34\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;129m@prompt\u001b[39m(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslate the given text to Chinese.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEXT: ```\u001b[39m\u001b[38;5;132;01m{text}\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39mopenai_model,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_to_chinese\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m foo: \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtranslate_to_chinese\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, how are you?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 139\u001b[0m, in \u001b[0;36mOpenAIPromptFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m bound_args\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBound arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbound_args\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFormatted messages: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessages\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model()\n",
      "Cell \u001b[0;32mIn[54], line 161\u001b[0m, in \u001b[0;36mOpenAIPromptFunction._format\u001b[0;34m(self, messages, args)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_messages:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [message_template\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mbound_args\u001b[49m\u001b[38;5;241m.\u001b[39marguments) \u001b[38;5;28;01mfor\u001b[39;00m message_template \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_messages]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bound_args' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI as LangchainChatOpenAI\n",
    "from langchain_openai.chat_models import AzureChatOpenAI as LangchainAzureChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "openai_model = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    ")\n",
    "\n",
    "langchain_openai_model = LangchainAzureChatOpenAI(\n",
    "    api_key=os.getenv(\"COMPANION_PROXY_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"COMPANION_PROXY_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"COMPANION_PROXY_API_VERSION\"),\n",
    "    model=os.getenv(\"COMPANION_PROXY_MODEL_NAME\"),\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "\n",
    "class foo(BaseModel):\n",
    "    bar: str\n",
    "\n",
    "\n",
    "@prompt(\n",
    "    \"Translate the given text to Chinese.\\n\",\n",
    "    \"TEXT: ```{text}```\",\n",
    "    model=openai_model,\n",
    ")\n",
    "def translate_to_chinese(text: str) -> foo: ...\n",
    "\n",
    "\n",
    "translate_to_chinese(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function signature: (text: str) -> __main__.foo\n",
      "Function parameters: odict_values([<Parameter \"text: str\">])\n",
      "Function return type: <class '__main__.foo'>\n",
      "Calling translate_to_chinese with `('Hello, how are you?',)` and `{}`\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OpenAIPromptFunction' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 34\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;129m@prompt\u001b[39m(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslate the given text to Chinese.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEXT: ```\u001b[39m\u001b[38;5;132;01m{text}\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39mopenai_model,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_to_chinese\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m foo: \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtranslate_to_chinese\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, how are you?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 133\u001b[0m, in \u001b[0;36mOpenAIPromptFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` and `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_messages)\n\u001b[0;32m--> 133\u001b[0m model \u001b[38;5;241m=\u001b[39m cast(OpenAI, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mchat()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OpenAIPromptFunction' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI as LangchainChatOpenAI\n",
    "from langchain_openai.chat_models import AzureChatOpenAI as LangchainAzureChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "openai_model = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    ")\n",
    "\n",
    "langchain_openai_model = LangchainAzureChatOpenAI(\n",
    "    api_key=os.getenv(\"COMPANION_PROXY_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"COMPANION_PROXY_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"COMPANION_PROXY_API_VERSION\"),\n",
    "    model=os.getenv(\"COMPANION_PROXY_MODEL_NAME\"),\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "\n",
    "class foo(BaseModel):\n",
    "    bar: str\n",
    "\n",
    "\n",
    "@prompt(\n",
    "    \"Translate the given text to Chinese.\",\n",
    "    \"TEXT: ```{text}```\",\n",
    "    model=openai_model,\n",
    ")\n",
    "def translate_to_chinese(text: str) -> foo: ...\n",
    "\n",
    "\n",
    "translate_to_chinese(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
